---
title: "Power Analysis for Time-Course Phosphoproteomics (DDA)"
author: "peppwR"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
    toc: true
    toc_float: true
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6
)
set.seed(42)
```

## Introduction

### What is Statistical Power?

Statistical power is the probability of detecting a true effect when one exists. In proteomics experiments, this translates to: "If a phosphopeptide truly changes between conditions, how likely are we to find it statistically significant?"

Power depends on four interconnected factors:

1. **Effect size** - How large is the biological change? (e.g., 2-fold)
2. **Sample size** - How many biological replicates per group?
3. **Variability** - How noisy is the measurement?
4. **Significance threshold** - What alpha level are we using? (typically 0.05)

### Why Does Power Matter?

Phosphoproteomics experiments are expensive and time-consuming. Under-powered experiments waste resources by failing to detect real biological changes. Over-powered experiments waste resources by using more samples than necessary.

Power analysis helps researchers:

- **Before an experiment:** Determine required sample size for adequate power
- **After a pilot:** Assess what effects can be detected with current data
- **For grant planning:** Justify sample size requirements to reviewers

### The peppwR Approach

peppwR uses simulation-based power analysis:

1. **Fit distributions** to pilot data to capture realistic abundance patterns
2. **Simulate experiments** by drawing from fitted distributions
3. **Apply statistical tests** to simulated data
4. **Estimate power** as the proportion of simulations with significant results

This approach captures the full complexity of phosphoproteomics data, including heterogeneity across peptides.

## About the Data

This analysis uses Arabidopsis phosphoproteomics data from a time-course experiment:

- **Method:** DDA (Data-Dependent Acquisition) mass spectrometry
- **Comparison:** Early (t=0) vs late (t=600) timepoints
- **Replicates:** 3 biological replicates per condition
- **Peptides:** 2,228 phosphopeptides quantified
- **Missing data:** None (complete observations)

## Data Preparation

```{r load-packages}
library(peppwR)
library(dplyr)
library(ggplot2)
```

```{r load-data}
# Load the DDA experiment data
dda <- read.csv("../../sample_data/dda_data.csv")

# Examine the structure
glimpse(dda)
```

```{r filter-data}
# Filter to early (0) vs late (600) timepoints and format for peppwR
pilot <- dda |>
  filter(timepoints %in% c(0, 600)) |>
  transmute(
    peptide_id = new_annotation,
    condition = paste0("t", timepoints),
    abundance = timepoints_values
  )

# Summary statistics
cat("Unique peptides:", n_distinct(pilot$peptide_id), "\n")
cat("Observations per condition:\n")
pilot |> count(condition)
```

```{r eda-plot, fig.cap="Distribution of log2 abundance values across conditions. The broad range and right-skew are typical for phosphoproteomics data."}
# Exploratory visualization
ggplot(pilot, aes(x = log2(abundance), fill = condition)) +
  geom_density(alpha = 0.5) +
  labs(
    x = "Log2 Abundance",
    y = "Density",
    title = "Phosphopeptide Abundance Distribution"
  ) +
  theme_minimal()
```

The abundance values span several orders of magnitude, typical for phosphoproteomics data. The distributions appear similar between timepoints at the global level, though individual peptides may show significant changes.

## Distribution Fitting

### Why Fit Distributions?

To simulate realistic experiments, we need parametric models that capture the statistical properties of each peptide's abundance values. peppwR fits multiple candidate distributions (gamma, lognormal, inverse Gaussian, etc.) and selects the best fit for each peptide based on AIC.

This per-peptide approach respects the heterogeneity of phosphoproteomics data - different peptides can have different underlying distributions.

```{r fit-distributions}
# Fit distributions to each peptide
fits <- fit_distributions(pilot, "peptide_id", "condition", "abundance")

# Summary of fitting results
print(fits)
```

```{r plot-fits, fig.cap="Best-fit distribution counts across the peptidome. This shows which parametric distributions best describe phosphopeptide abundance data."}
# Visualize which distributions fit best
plot(fits)
```

The fitting results reveal that most phosphopeptides are best described by lognormal or gamma distributions - consistent with the positive, right-skewed nature of mass spectrometry intensities.

### Parameter Distribution

Understanding how fitted parameters vary across the peptidome helps assess whether our simulation assumptions are reasonable.

```{r param-dist, fig.cap="Distribution of AIC values across peptides for each fitted distribution. Lower AIC indicates better fit. Note that some distributions may fit only a small subset of peptides."}
p <- plot_param_distribution(fits)
print(p)

# Count peptides per best-fit distribution for context
cat("\nPeptides per best-fit distribution:\n")
tibble::tibble(distribution = fits$best) |>
  count(distribution) |>
  arrange(desc(n))
```

When a distribution shows a sparse histogram (few bars), it means that distribution was selected as best fit for only a small number of peptides. This is normal - it simply reflects which distributional shapes occur in the data.

## Power Analysis

### The Three Questions

Power analysis can answer three related questions about experimental design:

1. **Power:** Given sample size and effect size, what is my power?
2. **Sample size:** Given target power and effect size, what N do I need?
3. **Minimum detectable effect:** Given sample size and target power, what's the smallest effect I can detect?

These questions are mathematically related - fixing any three parameters determines the fourth.

### Question 1: Current Power (N=3)

With 3 biological replicates per group (as in this experiment), what power do we have to detect a 2-fold change?

```{r power-q1}
# Use n_sim = 100 for faster rendering in this example
power_n3 <- power_analysis(fits, effect_size = 2, n_per_group = 3, find = "power", n_sim = 100)
print(power_n3)
```

```{r plot-power-n3, fig.cap="Distribution of power across peptides with N=3 and 2-fold effect. Each bar represents the proportion of peptides achieving that power level."}
plot(power_n3)
```

With N=3, power to detect a 2-fold change varies widely across peptides. Peptides with lower variability will have higher power, while noisy peptides remain under-powered.

### Question 2: Sample Size for Target Power

What sample size would we need to achieve 80% power to detect a 1.5-fold change?

```{r power-q2}
sample_size <- power_analysis(fits, effect_size = 1.5, target_power = 0.8, find = "sample_size", n_sim = 100)
print(sample_size)
```

```{r plot-sample-size, fig.cap="Percentage of peptides reaching 80% power at each sample size. The curve shows diminishing returns as N increases."}
plot(sample_size)
```

This curve directly answers "what percentage of my peptides will be well-powered at sample size N?" - useful for experimental planning when budget constraints limit replication.

### Question 3: Minimum Detectable Effect

At N=3 with 80% power target, what's the smallest effect we can reliably detect?

```{r power-q3}
min_effect <- power_analysis(fits, n_per_group = 3, target_power = 0.8, find = "effect_size", n_sim = 100)
print(min_effect)
```

```{r plot-min-effect, fig.cap="Distribution of minimum detectable effect sizes across peptides. Larger values indicate peptides that require stronger effects to be detected."}
plot(min_effect)
```

With only N=3, the minimum detectable effect for most peptides is quite large. This is a realistic assessment of what under-powered experiments can and cannot detect.

### Comparing Statistical Tests

The choice of statistical test can affect power. peppwR supports several tests:

- **Wilcoxon rank-sum** (default): Non-parametric, robust to outliers
- **Bootstrap-t**: Handles non-normality through resampling
- **Bayes factor t-test**: Bayesian evidence for an effect

Let's compare power across these tests.

```{r compare-tests}
# For test comparison, use a subset of peptides for faster computation
# In practice, you would run this on the full dataset

# Sample 200 peptides for comparison
set.seed(123)
sample_ids <- sample(unique(pilot$peptide_id), min(200, n_distinct(pilot$peptide_id)))
pilot_sample <- pilot |> filter(peptide_id %in% sample_ids)
fits_sample <- fit_distributions(pilot_sample, "peptide_id", "condition", "abundance")

# Wilcoxon (default)
power_wilcox <- power_analysis(fits_sample, effect_size = 2, n_per_group = 3,
                                find = "power", test = "wilcoxon", n_sim = 100)

# Bootstrap-t
power_boot <- power_analysis(fits_sample, effect_size = 2, n_per_group = 3,
                              find = "power", test = "bootstrap_t", n_sim = 100)

# Bayes factor t-test
power_bayes <- power_analysis(fits_sample, effect_size = 2, n_per_group = 3,
                               find = "power", test = "bayes_t", n_sim = 100)

# Summary comparison
cat("Median power by test (N=3, 2-fold effect, 200-peptide sample):\n")
cat("  Wilcoxon rank-sum:", round(median(power_wilcox$simulations$peptide_power, na.rm = TRUE), 3), "\n")
cat("  Bootstrap-t:      ", round(median(power_boot$simulations$peptide_power, na.rm = TRUE), 3), "\n")
cat("  Bayes factor:     ", round(median(power_bayes$simulations$peptide_power, na.rm = TRUE), 3), "\n")
```

Different tests may perform better for different data types. Wilcoxon is a robust default for proteomics data where distributional assumptions may not hold perfectly.

## Power Heatmap

A power heatmap visualizes how power varies across combinations of sample size and effect size - useful for identifying the "sweet spot" for experimental design.

```{r power-heatmap, fig.cap="Power as a function of sample size and effect size. Colors indicate expected power. Use this to plan experiments by finding acceptable combinations of N and target effect."}
# Use typical lognormal parameters for aggregate simulation
plot_power_heatmap(
  distribution = "lnorm",
  params = list(meanlog = 12, sdlog = 1.5),
  n_range = c(3, 12),
  effect_range = c(1.2, 3)
)
```

The heatmap shows:

- **Lower-left corner** (small N, small effect): Low power - avoid this region
- **Upper-right corner** (large N, large effect): High power but potentially wasteful
- **Diagonal transition zone**: The practical planning region where tradeoffs matter

## Summary and Recommendations

### Key Findings

1. **Distribution fitting:** Phosphopeptide abundances in this DDA dataset are predominantly lognormal or gamma distributed, consistent with typical mass spectrometry data.

2. **Current power (N=3):** With 3 replicates, power to detect a 2-fold change is moderate. Many peptides have power below 80%, especially those with higher variability.

3. **Sample size requirements:** Achieving 80% power for a 1.5-fold change requires more than N=3 for most peptides. The exact requirement varies by peptide.

4. **Minimum detectable effect:** At N=3, only relatively large fold changes (often > 2-fold) can be reliably detected at 80% power.

### Practical Recommendations

1. **For future experiments:** Consider N=6 or more biological replicates if detecting 1.5-fold changes is important.

2. **For current data interpretation:** Results showing non-significance may reflect insufficient power rather than absence of biological effect. Be cautious about concluding "no difference."

3. **Test selection:** Wilcoxon rank-sum is a reasonable default. Consider bootstrap-t if computational resources allow.

### Caveats

- This analysis uses a single genotype (Col-0) and specific timepoint comparison (0 vs 600)
- Power estimates assume fitted distributions accurately represent underlying biology
- Technical variability (not modeled here) may further reduce effective power
- Real experimental factors (batch effects, sample quality) are not captured

## Session Info

```{r session-info}
sessionInfo()
```

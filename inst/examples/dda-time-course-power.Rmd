---
title: "Power Analysis for Time-Course Phosphoproteomics (DDA)"
author: "peppwR"
date: "`r Sys.Date()`"
output:
  html_document:
    self_contained: true
    toc: true
    toc_float: true
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6
)
set.seed(42)
```

## Introduction

### What is Statistical Power?

Statistical power is the probability of detecting a true effect when one exists. In proteomics experiments, this translates to: "If a phosphopeptide truly changes between conditions, how likely are we to find it statistically significant?"

Power depends on four interconnected factors:

1. **Effect size** - How large is the biological change? (e.g., 2-fold)
2. **Sample size** - How many biological replicates per group?
3. **Variability** - How noisy is the measurement?
4. **Significance threshold** - What alpha level are we using? (typically 0.05)

### Why Does Power Matter?

Phosphoproteomics experiments are expensive and time-consuming. Under-powered experiments waste resources by failing to detect real biological changes. Over-powered experiments waste resources by using more samples than necessary.

Power analysis helps researchers:

- **Before an experiment:** Determine required sample size for adequate power
- **After a pilot:** Assess what effects can be detected with current data
- **For grant planning:** Justify sample size requirements to reviewers

### The peppwR Approach

peppwR uses simulation-based power analysis:

1. **Fit distributions** to pilot data to capture realistic abundance patterns
2. **Simulate experiments** by drawing from fitted distributions
3. **Apply statistical tests** to simulated data
4. **Estimate power** as the proportion of simulations with significant results

This approach captures the full complexity of phosphoproteomics data, including heterogeneity across peptides.

## About the Data

This analysis uses Arabidopsis phosphoproteomics data from a time-course experiment:

| Property | Value |
|----------|-------|
| Method | DDA (Data-Dependent Acquisition) mass spectrometry |
| Comparison | Early (t=0) vs late (t=600) timepoints |
| Biological replicates | 3 per condition |
| Total observations per peptide | 6 (important limitation!) |
| Peptides | 2,228 phosphopeptides quantified |
| Missing data | None (complete observations) |

## Data Preparation

```{r load-packages}
library(peppwR)
library(dplyr)
library(ggplot2)
library(tibble)
```

```{r load-data}
# Load the DDA experiment data
dda <- read.csv("../../sample_data/dda_data.csv")

# Examine the structure
glimpse(dda)
```

```{r filter-data}
# Filter to early (0) vs late (600) timepoints and format for peppwR
pilot <- dda |>
  filter(timepoints %in% c(0, 600)) |>
  transmute(
    peptide_id = new_annotation,
    condition = paste0("t", timepoints),
    abundance = timepoints_values
  )

# Summary statistics
cat("Unique peptides:", n_distinct(pilot$peptide_id), "\n")
cat("Observations per condition:\n")
pilot |> count(condition)
```

```{r eda-plot, fig.cap="Distribution of log2 abundance values across conditions. The broad range and right-skew are typical for phosphoproteomics data."}
# Exploratory visualization
ggplot(pilot, aes(x = log2(abundance), fill = condition)) +
  geom_density(alpha = 0.5) +
  labs(
    x = "Log2 Abundance",
    y = "Density",
    title = "Phosphopeptide Abundance Distribution"
  ) +
  theme_minimal()
```

The abundance values span several orders of magnitude, typical for phosphoproteomics data. The distributions appear similar between timepoints at the global level, though individual peptides may show significant changes.

## Distribution Fitting

### Why Fit Distributions?

To simulate realistic experiments, we need parametric models that capture the statistical properties of each peptide's abundance values. peppwR fits multiple candidate distributions (gamma, lognormal, inverse Gaussian, Pareto, skew normal, etc.) and selects the best fit for each peptide based on AIC.

```{r fit-distributions}
# Fit distributions to each peptide
fits <- fit_distributions(pilot, "peptide_id", "condition", "abundance")

# Summary of fitting results
print(fits)
```

```{r plot-fits, fig.cap="Best-fit distribution counts across the peptidome."}
# Visualize which distributions fit best
plot(fits)
```

### Interpreting Distribution Fitting Results: A Cautionary Note

**Important:** With only 6 observations per peptide (3 replicates x 2 conditions), distribution selection is unreliable. You may observe that distributions like Pareto or Skew Normal dominate the "best fit" counts. This is an **artifact of small sample size**, not a statement about the true underlying distributions.

With more biological replicates, we would expect gamma and lognormal distributions to fit better - these are the typical distributions for mass spectrometry abundance data based on the underlying measurement process.

**Key insight:** The specific distribution matters less than having enough data to fit it reliably. For power analysis, we proceed with the best available fits while acknowledging this limitation.

### Parameter Distribution

```{r param-dist, fig.cap="Distribution of AIC values across peptides for each fitted distribution. Lower AIC indicates better fit."}
p <- plot_param_distribution(fits)
print(p)

# Count peptides per best-fit distribution for context
cat("\nPeptides per best-fit distribution:\n")
tibble(distribution = fits$best) |>
  count(distribution) |>
  arrange(desc(n))
```

## Power Analysis

### Choosing the Right Statistical Test

**Critical first step:** With small samples (N=3), the choice of statistical test dramatically affects power. Before proceeding with detailed analysis, we must compare tests to understand their behavior.

peppwR supports several tests:

| Test | Type | Characteristics |
|------|------|-----------------|
| Wilcoxon rank-sum | Non-parametric | Conservative, robust to outliers, needs larger N |
| Bootstrap-t | Resampling | Handles non-normality through resampling |
| Bayes factor | Bayesian | Evidence-based, often more powerful at small N |

Let's compare all three tests at N=3 with a 2-fold effect:

```{r test-comparison}
# Run all three tests (use n_sim = 100 for faster rendering)
power_wilcox <- power_analysis(fits, effect_size = 2, n_per_group = 3,
                                find = "power", test = "wilcoxon", n_sim = 100)

power_boot <- power_analysis(fits, effect_size = 2, n_per_group = 3,
                              find = "power", test = "bootstrap_t", n_sim = 100)

power_bayes <- power_analysis(fits, effect_size = 2, n_per_group = 3,
                               find = "power", test = "bayes_t", n_sim = 100)

# Create comparison table
comparison <- tibble(
  Test = c("Wilcoxon rank-sum", "Bootstrap-t", "Bayes factor"),
  `Median Power` = c(
    median(power_wilcox$simulations$peptide_power, na.rm = TRUE),
    median(power_boot$simulations$peptide_power, na.rm = TRUE),
    median(power_bayes$simulations$peptide_power, na.rm = TRUE)
  ),
  `% > 50% Power` = c(
    mean(power_wilcox$simulations$peptide_power > 0.5, na.rm = TRUE) * 100,
    mean(power_boot$simulations$peptide_power > 0.5, na.rm = TRUE) * 100,
    mean(power_bayes$simulations$peptide_power > 0.5, na.rm = TRUE) * 100
  ),
  `% > 80% Power` = c(
    mean(power_wilcox$simulations$peptide_power > 0.8, na.rm = TRUE) * 100,
    mean(power_boot$simulations$peptide_power > 0.8, na.rm = TRUE) * 100,
    mean(power_bayes$simulations$peptide_power > 0.8, na.rm = TRUE) * 100
  )
)

knitr::kable(comparison, digits = 2,
             caption = "Power comparison across statistical tests (N=3, 2-fold effect)")
```

### Understanding the Test Comparison Results

The comparison table reveals important insights:

- **Wilcoxon rank-sum** is conservative with small samples. Non-parametric tests don't make distributional assumptions, but this comes at a cost: they need more data to achieve comparable power.

- **Bootstrap-t** uses resampling to handle non-normality, potentially offering better power than Wilcoxon.

- **Bayes factor** tests provide evidence for or against an effect. At small N, Bayesian approaches can be more informative than frequentist tests.

**Recommendation:** For the remaining analyses in this document, we use the Bayes factor test since it provides usable power estimates at N=3.

### The Three Questions

Power analysis can answer three related questions about experimental design:

1. **Power:** Given sample size and effect size, what is my power?
2. **Sample size:** Given target power and effect size, what N do I need?
3. **Minimum detectable effect:** Given sample size and target power, what's the smallest effect I can detect?

These questions are mathematically related - fixing any three parameters determines the fourth.

### Question 1: Current Power (N=3)

With 3 biological replicates per group (as in this experiment), what power do we have to detect a 2-fold change?

```{r power-q1}
# Using Bayes factor test based on our comparison results
power_n3 <- power_analysis(fits, effect_size = 2, n_per_group = 3,
                           find = "power", test = "bayes_t", n_sim = 100)
print(power_n3)
```

```{r plot-power-n3, fig.cap="Distribution of power across peptides with N=3 and 2-fold effect (Bayes factor test). Each bar represents the proportion of peptides achieving that power level."}
plot(power_n3)
```

With N=3, power to detect a 2-fold change varies across peptides. Peptides with lower variability will have higher power, while noisy peptides remain under-powered.

### Question 2: Sample Size for Target Power

What sample size would we need to achieve 80% power to detect a 2-fold change for most peptides?

```{r power-q2}
sample_size <- power_analysis(fits, effect_size = 2, target_power = 0.8,
                              find = "sample_size", test = "bayes_t", n_sim = 100)
print(sample_size)
```

```{r plot-sample-size, fig.cap="Percentage of peptides reaching 80% power at each sample size. The curve shows diminishing returns as N increases."}
plot(sample_size)
```

This curve directly answers "what percentage of my peptides will be well-powered at sample size N?" - useful for experimental planning when budget constraints limit replication.

### Question 3: Minimum Detectable Effect

At N=3 with 80% power target, what's the smallest effect we can reliably detect?

```{r power-q3}
min_effect <- power_analysis(fits, n_per_group = 3, target_power = 0.8,
                             find = "effect_size", test = "bayes_t", n_sim = 100)
print(min_effect)
```

```{r plot-min-effect, fig.cap="Distribution of minimum detectable effect sizes across peptides. Larger values indicate peptides that require stronger effects to be detected."}
plot(min_effect)
```

With only N=3, the minimum detectable effect for many peptides is quite large. This is a realistic assessment of what under-powered experiments can and cannot detect.

## Power Heatmap

A power heatmap visualizes how power varies across combinations of sample size and effect size - useful for identifying the "sweet spot" for experimental design.

### Why Aggregate Mode for Heatmaps?

The `plot_power_heatmap()` function uses **aggregate mode** (single distribution) rather than per-peptide mode because:

1. **Computational cost**: Per-peptide heatmaps would require millions of simulations
2. **Visualization**: A single heatmap is more interpretable than 2000+ individual heatmaps
3. **Purpose**: Heatmaps answer "what's the general tradeoff?" not "what's the power for peptide X?"

To make the heatmap relevant to your data, we derive representative parameters from the fitted distributions:

```{r power-heatmap, fig.cap="Power as a function of sample size and effect size for a representative peptide. Colors indicate expected power. Individual peptides will vary based on their specific variability."}
# Derive representative parameters from the data
log_abundances <- log(pilot$abundance[pilot$abundance > 0])
derived_params <- list(
  meanlog = median(log_abundances),
  sdlog = mad(log_abundances, constant = 1)  # robust SD estimate
)

cat("Using representative lognormal parameters:\n")
cat("  meanlog =", round(derived_params$meanlog, 2), "\n")
cat("  sdlog =", round(derived_params$sdlog, 2), "\n")

# Generate heatmap with data-derived parameters
plot_power_heatmap(
  distribution = "lnorm",
  params = derived_params,
  n_range = c(3, 12),
  effect_range = c(1.2, 3),
  test = "wilcoxon"  # heatmap uses aggregate mode
)
```

### Interpreting the Heatmap

The heatmap shows power for a **representative peptide** with typical abundance characteristics. Individual peptides will vary:

- Low-variability peptides will have higher power than shown
- High-variability peptides will have lower power than shown
- Use the per-peptide `find = "power"` results to understand the distribution across peptides

The heatmap shows:

- **Lower-left corner** (small N, small effect): Low power - avoid this region
- **Upper-right corner** (large N, large effect): High power but potentially wasteful
- **Diagonal transition zone**: The practical planning region where tradeoffs matter

## Summary and Recommendations

### Key Findings

1. **Distribution fitting:** With only 6 observations per peptide, distribution selection is unreliable. The observed "best fit" distributions are artifacts of small sample size, not the true underlying distributions.

2. **Test selection matters:** Wilcoxon rank-sum may have limited power at N=3 due to its conservative nature. Bayes factor tests can provide more informative power estimates with small samples.

3. **Current power (N=3):** With 3 replicates and the Bayes factor test, power to detect a 2-fold change varies across peptides based on their individual variability.

4. **Sample size requirements:** Achieving consistently high power (80%+) across most peptides requires more than N=3 biological replicates.

5. **Minimum detectable effect:** At N=3, only relatively large fold changes can be reliably detected at 80% power for most peptides.

### Practical Recommendations

1. **For future experiments:** Consider N=6 or more biological replicates if detecting smaller fold changes (<2-fold) is important.

2. **For current data interpretation:** Results showing non-significance may reflect insufficient power rather than absence of biological effect. Be cautious about concluding "no difference."

3. **Test selection:** With small samples, consider Bayes factor tests which may be more informative than traditional frequentist tests.

4. **Effect size expectations:** Set realistic expectations - detecting subtle changes requires adequate replication.

### Caveats

- This analysis uses a single genotype (Col-0) and specific timepoint comparison (0 vs 600)
- Distribution fitting is limited by small sample size (6 observations per peptide)
- Power estimates assume fitted distributions accurately represent underlying biology
- Technical variability (not modeled here) may further reduce effective power
- Real experimental factors (batch effects, sample quality) are not captured

## Session Info

```{r session-info}
sessionInfo()
```
